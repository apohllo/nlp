{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29cab7ec",
   "metadata": {},
   "source": [
    "\n",
    "# **Laboratorium 002: Skuteczne promptowanie modeli LLM w Ollama**\n",
    "\n",
    "## **Cel zajÄ™Ä‡**\n",
    "\n",
    "- WyÄ‡wiczenie praktycznych technik **prompt engineeringu** na lokalnych modelach uruchamianych przez **Ollama**.  \n",
    "- Zastosowanie **struktur i rÃ³l** (system/developer, user, assistant) w praktyce â€“ z naciskiem na *kompozycjÄ™ promptÃ³w*, a nie pojedyncze pytania.  \n",
    "- Przeprowadzenie **ewaluacji promptÃ³w** (PromptFoo) i analizy zachowania modeli.  \n",
    "- Zrozumienie zasad **bezpieczeÅ„stwa, prompt injection, anonimizacji** i dobrych praktyk projektowych.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Przygotowanie Å›rodowiska i prerekwizyty**\n",
    "\n",
    "- Zainstalowana **Ollama** i dwa modele w konwencji *polski â€“ niepolski* (`llama3.1:8b`, `gemma2:2b`, `SpeakLeash/bielik-*`).  \n",
    "\n",
    "Test dziaÅ‚ania modelu:\n",
    "\n",
    "```bash\n",
    "echo \"Napisz rymowankÄ™ o najlepszym wykÅ‚adowcy na WI.\" | ollama run gemma2:2b\n",
    "```\n",
    "---\n",
    "\n",
    "- MoÅ¼emy teÅ¼ przenieÅ›Ä‡ siÄ™ z pracÄ… do notebooka, wtedy nasz warsztat bÄ™dzie wyglÄ…daÅ‚ tak:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189efaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(\n",
    "    model='gemma2:2b',\n",
    "    messages=[{'role': 'user', 'content': 'Napisz rymowankÄ™ o najlepszym wykÅ‚adowcy na WI'}]\n",
    ")\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee97015f",
   "metadata": {},
   "source": [
    "## **2. Struktura i techniki promptowania**\n",
    "\n",
    "### 2.1 Zero-shot\n",
    "\n",
    "**Zeroâ€‘shot prompting** to sposÃ³b formuÅ‚owania zapytania do duÅ¼ego modelu jÄ™zykowego, w ktÃ³rym nie podajemy mu ani jednego przykÅ‚adu wykonanego zadania. Model dostaje wyÅ‚Ä…cznie instrukcjÄ™ (â€zrÃ³b Xâ€) i caÅ‚Ä… resztÄ™ dedukuje sam na podstawie wiedzy nabytej w treningu.\n",
    "\n",
    "Zadania, do ktÃ³rych uÅ¼ywa siÄ™ **zero-shot prompting**:\n",
    "1. Szybkie prototypowanie â€“ gdy chcemy sprawdziÄ‡, czy model moÅ¼e poradziÄ‡ sobie z danym zadaniem (czy jego architektura i materiaÅ‚ treningowy na to pozwalajÄ…).\n",
    "\n",
    "2. Zadania ogÃ³lne o prostym formacie wyjÅ›ciowym (tÅ‚umaczenia, parafrazy, proste klasyfikacje).\n",
    "\n",
    "3. Systemy low-code/no-code â€“ uÅ¼ytkownik pisze naturalnym jÄ™zykiem, a model odpowiada bez konfiguracji.\n",
    "\n",
    "4. Ekstrakcja informacji _ad hoc_ (imiÄ™, mail, daty z tekstu itp.) tam, gdzie nie opÅ‚aca siÄ™ trenowaÄ‡ lub fine-tune'owaÄ‡ modelu.\n",
    "\n",
    "\n",
    "Kiedy **zero-shot** jest najbardziej efektywne?\n",
    "- brak lub szczÄ…tkowe dane oznaczone â€“ nie mamy przykÅ‚adowych par wejÅ›cieâ†’wyjÅ›cie.\n",
    "- zadanie jest podobne do scenariuszy widzianych w treningu (np. â€podsumujâ€, â€przetÅ‚umaczâ€, â€odpowiedz TAK/NIEâ€).\n",
    "- liczy siÄ™ czas wdroÅ¼enia â€“ chcemy odpowiedzi â€tu i terazâ€, bez skrupulatnego modyfikowania promptu.\n",
    "\n",
    "Kiedy nie wystarcza?\n",
    "- zÅ‚oÅ¼one transformacje wymagajÄ…ce Å›cisÅ‚ej struktury lub niestandardowych reguÅ‚.\n",
    "- zadania domenowe z maÅ‚o popularnym Å¼argonem (medycyna, prawo) â€“ wtedy lepszy jest few-shot lub RAG.\n",
    "\n",
    "```bash\n",
    "echo \"Wypisz 5 sÅ‚Ã³w kluczowych z notatki o spotkaniu KoÅ‚a Naukowego BIT.\" | ollama run gemma2:2b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2a383c",
   "metadata": {},
   "source": [
    "ğŸ”µ Spotkanie KoÅ‚a Naukowego BIT\n",
    "\n",
    "ğŸ“… Åšroda, godz. 14:00\n",
    "ğŸ“ Sala 4.30D, WydziaÅ‚ Informatyki\n",
    "\n",
    "Serdecznie zapraszamy wszystkich studentÃ³w â€” zarÃ³wno obecnych czÅ‚onkÃ³w, jak i osoby, ktÃ³re dopiero chcÄ… doÅ‚Ä…czyÄ‡ â€” na kolejne spotkanie KoÅ‚a Naukowego BIT!\n",
    "\n",
    "ğŸ’¡ Plan spotkania:\n",
    "\n",
    "- OmÃ³wienie planu projektÃ³w na semestr zimowy â€“ propozycje tematÃ³w z zakresu AI, bezpieczeÅ„stwa i tworzenia aplikacji webowych.\n",
    "\n",
    "- Prezentacja aktualnych inicjatyw â€“ praca nad systemem rozpoznawania obrazu oraz chatbotem opartym na modelach LLM.\n",
    "\n",
    "- Warsztaty wprowadzajÄ…ce â€“ krÃ³tkie zajÄ™cia praktyczne z narzÄ™dzia Ollama i prompt engineeringu.\n",
    "\n",
    "- PodziaÅ‚ na zespoÅ‚y projektowe â€“ moÅ¼liwoÅ›Ä‡ zapisania siÄ™ do wybranego projektu.\n",
    "\n",
    " - Dyskusja i networking â€“ poznaj ludzi, ktÃ³rzy programujÄ… z pasjÄ…!\n",
    "\n",
    "ğŸš€ Dlaczego warto doÅ‚Ä…czyÄ‡?\n",
    "\n",
    "- Rozwijasz praktyczne umiejÄ™tnoÅ›ci techniczne (AI, ML, web, backend).\n",
    "\n",
    "- Bierzesz udziaÅ‚ w projektach badawczo-rozwojowych i hackathonach.\n",
    "\n",
    "- Zyskujesz doÅ›wiadczenie zespoÅ‚owe i wsparcie mentorÃ³w z branÅ¼y.\n",
    "\n",
    "- Spotykasz ludzi, ktÃ³rzy chcÄ… robiÄ‡ coÅ› wiÄ™cej niÅ¼ tylko zaliczaÄ‡ przedmioty.\n",
    "\n",
    "ğŸ“¢ Nie musisz mieÄ‡ doÅ›wiadczenia â€” wystarczy ciekawoÅ›Ä‡ i chÄ™Ä‡ nauki!\n",
    "PrzyjdÅº w Å›rodÄ™ o 14:00 do sali 4.30D i zobacz, czym Å¼yje nasze koÅ‚o!\n",
    "\n",
    "âœ‰ï¸ W razie pytaÅ„: bit@wi.edu.pl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffd0875",
   "metadata": {},
   "source": [
    "Zadanie 2.1.1 Generowanie streszczenia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08241491",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "ğŸ”µ Spotkanie KoÅ‚a Naukowego BIT\n",
    "\n",
    "ğŸ“… Åšroda, godz. 14:00\n",
    "ğŸ“ Sala 4.30D, WydziaÅ‚ Informatyki\n",
    "\n",
    "Serdecznie zapraszamy wszystkich studentÃ³w â€” zarÃ³wno obecnych czÅ‚onkÃ³w, jak i osoby, ktÃ³re dopiero chcÄ… doÅ‚Ä…czyÄ‡ â€” na kolejne spotkanie KoÅ‚a Naukowego BIT!\n",
    "\n",
    "ğŸ’¡ Plan spotkania:\n",
    "\n",
    "OmÃ³wienie planu projektÃ³w na semestr zimowy â€“ propozycje tematÃ³w z zakresu AI, bezpieczeÅ„stwa i tworzenia aplikacji webowych.\n",
    "\n",
    "Prezentacja aktualnych inicjatyw â€“ praca nad systemem rozpoznawania obrazu oraz chatbotem opartym na modelach LLM.\n",
    "\n",
    "Warsztaty wprowadzajÄ…ce â€“ krÃ³tkie zajÄ™cia praktyczne z narzÄ™dzia Ollama i prompt engineeringu.\n",
    "\n",
    "PodziaÅ‚ na zespoÅ‚y projektowe â€“ moÅ¼liwoÅ›Ä‡ zapisania siÄ™ do wybranego projektu.\n",
    "\n",
    "Dyskusja i networking â€“ poznaj ludzi, ktÃ³rzy programujÄ… z pasjÄ…!\n",
    "\n",
    "ğŸš€ Dlaczego warto doÅ‚Ä…czyÄ‡?\n",
    "\n",
    "Rozwijasz praktyczne umiejÄ™tnoÅ›ci techniczne (AI, ML, web, backend).\n",
    "\n",
    "Bierzesz udziaÅ‚ w projektach badawczo-rozwojowych i hackathonach.\n",
    "\n",
    "Zyskujesz doÅ›wiadczenie zespoÅ‚owe i wsparcie mentorÃ³w z branÅ¼y.\n",
    "\n",
    "Spotykasz ludzi, ktÃ³rzy chcÄ… robiÄ‡ coÅ› wiÄ™cej niÅ¼ tylko zaliczaÄ‡ przedmioty.\n",
    "\n",
    "ğŸ“¢ Nie musisz mieÄ‡ doÅ›wiadczenia â€” wystarczy ciekawoÅ›Ä‡ i chÄ™Ä‡ nauki!\n",
    "PrzyjdÅº w Å›rodÄ™ o 14:00 do sali 4.30D i zobacz, czym Å¼yje nasze koÅ‚o!\n",
    "\n",
    "âœ‰ï¸ W razie pytaÅ„: bit@wi.edu.pl\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"Return a JSON with one key \"summary\" containing\n",
    "a 30â€‘word English summary of the following text:\n",
    "\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "response = ollama.chat(\n",
    "    model='gemma2:2b',\n",
    "    messages=[{'role': 'user', 'content': prompt}]\n",
    ")\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2507c02",
   "metadata": {},
   "source": [
    "Zadanie 2.1.2 Otrzymywanie odpowiedzi od modelu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2a76f6",
   "metadata": {},
   "source": [
    "Poprzednie laboratoria byÅ‚y poÅ›wiÄ™cone gÅ‚Ã³wnie zapytaniom typu zero-shot, ktÃ³rymi weryfikuje siÄ™ dziaÅ‚anie modeli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17301812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytanie weryfikujÄ…ce wiedzÄ™ i pewnoÅ›Ä‡ modelu\n",
    "response = ollama.chat(\n",
    "    model='gemma2:2b',\n",
    "    messages=[{'role': 'user', 'content': 'jaka jest pogoda w warszawie'}]\n",
    ")\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2238bf15",
   "metadata": {},
   "source": [
    "Zadanie 2.1.3\n",
    "\n",
    "UÅ¼ywajÄ…c odpowiednich technik promptowania, moÅ¼emy wymusiÄ‡ odpowiedÅº na modelu. PosÅ‚uÅ¼my siÄ™ wiÄ™c powyÅ¼szym przykÅ‚adem i rozbudujmy odpowiedÅº modelu, wykorzystujÄ…c schemat JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d0e34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class CityWeather(BaseModel):\n",
    "    city: str\n",
    "    temp_c: float\n",
    "    condition: str | None\n",
    "\n",
    "response = ollama.chat(\n",
    "    model='gemma2:2b',\n",
    "    messages=[{'role':'user','content':'Weather in Warsaw today.'}],\n",
    "    format=CityWeather.model_json_schema(),   # â† schema trafia do Ollamy\n",
    "    options={'temperature':0}\n",
    ")\n",
    "\n",
    "weather = CityWeather.model_validate_json(response['message']['content'])\n",
    "print(weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332d73b3",
   "metadata": {},
   "source": [
    "Podobne pytanie w PowerShellu:\n",
    "```PowerShell\n",
    "@'\n",
    "<ROLE>JesteÅ› asystentem danych. Zwracasz tylko JSON.</ROLE>\n",
    "<GOAL>WyodrÄ™bnij dane liczbowe i jednostki.</GOAL>\n",
    "<RULES>- JeÅ›li brak danych, zwrÃ³Ä‡ null.</RULES>\n",
    "<INPUT>Jan kupiÅ‚ 3 jabÅ‚ka po 2 zÅ‚.</INPUT>\n",
    "<OUTPUT>\n",
    "'@ | ollama run gemma2:2b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7aff2e",
   "metadata": {},
   "source": [
    "> **Pytania pomocnicze**  \n",
    "> - Jakiej odpowiedzi oczekiwaÅ‚eÅ›/aÅ›?  \n",
    "> - Czy model zachowaÅ‚ format JSON?  \n",
    "> - W jaki inny sposÃ³b moÅ¼na wymusiÄ‡ przestrzeganie formatu?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf59b95",
   "metadata": {},
   "source": [
    "Dodatkowe wskazÃ³wki:\n",
    "\n",
    "1. Nie otaczaj JSON-u blokiem json â€“ przy format=\"json\" to zbÄ™dne, a istnieje ryzyko, Å¼e model uzna ``` za legalny token i walidacja siÄ™ przerwie.\n",
    "\n",
    "2. Daj modelowi â€awaryjnÄ…â€ Å›cieÅ¼kÄ™ â€“ np. If you canâ€™t comply, output {}. Zmniejsza szansÄ™ na halucynacje.\n",
    "\n",
    "3. Limit dÅ‚ugoÅ›ci â€“ przy duÅ¼ych strukturach dodaj max_tokens, Å¼eby model nie wypadÅ‚ poza kontekst i nie zamknÄ…Å‚ nawiasu.\n",
    "\n",
    "4. Testuj â†’ parsuj â†’ prÃ³buj od nowa â€“ w kodzie API zawsze parsuj JSON i w razie JSONDecodeError ponawiaj z tym samym promptem; przy format='json' bÅ‚Ä™dy i tak zdarzajÄ… siÄ™ rzadko.\n",
    "\n",
    "5. UÅ¼ytkownik-czat â€“ gdy korzystasz interaktywnie (bez API), nie masz formatu; wtedy najpewniejszy miks to system-instrukcja + <json>â€¦</json> + temp=0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb97a47",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.2 Few-shot prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27edcb5e",
   "metadata": {},
   "source": [
    "DodajÄ…c **przykÅ‚ady** w promptach (na rÃ³Å¼nych poziomach), wskazujemy modelowi poÅ¼Ä…dany wzorzec.\n",
    "\n",
    "Zadanie 2.2.1  \n",
    "Przygotuj co najmniej dwa przykÅ‚ady (pytanieÂ â†’ odpowiedÅº) i poproÅ› model o wygenerowanie odpowiedzi dla nowego pytania w tym samym stylu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a762d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\"role\": \"user\", \"content\": \"Translate to emoji: I love programming\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"ğŸ’»â¤ï¸\"},\n",
    "    {\"role\": \"user\", \"content\": \"Translate to emoji: Fire and ice\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"ğŸ”¥â„ï¸\"},\n",
    "    {\"role\": \"user\", \"content\": \"Translate to emoji: Peace and coffee. Trophy and fast car. Running and winning.\"}\n",
    "]\n",
    "\n",
    "response = ollama.chat(model='gemma2:2b', messages=examples)\n",
    "print(response['message']['content'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb500f5",
   "metadata": {},
   "source": [
    "Zadanie 2.2.2 Ekstrakcja danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef9ca61",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\"role\": \"user\", \"content\": 'WejÅ›cie: \"Spotkanie w Å›rodÄ™ o 14:00 w sali 4.30D.\"'},\n",
    "    {\"role\": \"assistant\", \"content\": '{\"data\":\"Å›roda\",\"godzina\":\"14:00\",\"miejsce\":\"sala 4.30D\"}'},\n",
    "    {\"role\": \"user\", \"content\": 'WejÅ›cie: \"Jutro o 9:30 na Teamsach.\"'},\n",
    "    {\"role\": \"assistant\", \"content\": '{\"data\":\"jutro\",\"godzina\":\"09:30\",\"miejsce\":\"Teams\"}'},\n",
    "    {\"role\": \"user\", \"content\": 'WejÅ›cie: \"W czwartek po poÅ‚udniu w Warszawie, kawiarnia Relaks.\"'}\n",
    "]\n",
    "\n",
    "response = ollama.chat(model='gemma2:2b', messages=examples)\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e064211f",
   "metadata": {},
   "source": [
    "Podobna struktura zapytaÅ„ w PowerShellu:\n",
    "```bash\n",
    "\n",
    "@'\n",
    "PrzykÅ‚ad 1:\n",
    "WejÅ›cie: \"Spotkanie w Å›rodÄ™ o 14:00 w sali 4.30D.\"\n",
    "WyjÅ›cie: {\"data\":\"Å›roda\",\"godzina\":\"14:00\",\"miejsce\":\"sala 4.30D\"}\n",
    "\n",
    "PrzykÅ‚ad 2:\n",
    "WejÅ›cie: \"Jutro o 9:30 na Teamsach.\"\n",
    "WyjÅ›cie: {\"data\":\"jutro\",\"godzina\":\"09:30\",\"miejsce\":\"Teams\"}\n",
    "\n",
    "Nowe wejÅ›cie: \"W czwartek po poÅ‚udniu w Warszawie, kawiarnia Relaks.\"\n",
    "'@ | ollama run llama3.1:8b\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaa0809",
   "metadata": {},
   "source": [
    "Zadanie 2.2.3 Ekstrakcja danych do formatu JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2744a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "schema = \"\"\"Return valid JSON: { \"name\": <string>, \"title\": <string>, \"company\": <string> }\"\"\"\n",
    "\n",
    "shots = [\n",
    "    # przykÅ‚ad 1\n",
    "    {\"role\": \"user\",      \"content\": schema + \"\\nâ€” Sarah Connors-Newman â€” Director of Operations at Skynet Industries\"},\n",
    "    {\"role\": \"assistant\", \"content\": '{\"name\":\"Sarah Connors-Newman\",\"title\":\"Director of Operations\",\"company\":\"Skynet Industries\"}'},\n",
    "    # przykÅ‚ad 2\n",
    "    {\"role\": \"user\",      \"content\": schema + \"\\nâ€” Dr. Hiro Tanaka, Lead Scientist â€¢ QuantumX\"},\n",
    "    {\"role\": \"assistant\", \"content\": '{\"name\":\"Hiro Tanaka\",\"title\":\"Lead Scientist\",\"company\":\"QuantumX\"}'},\n",
    "    # przykÅ‚ad 3\n",
    "    {\"role\": \"user\",      \"content\": schema + \"\\nâ€” Prof. dr hab. Janek Dzbanek Przewodnik Wszystkich AGH\"},\n",
    "    {\"role\": \"assistant\", \"content\": '{\"name\":\"Janek Dzbanek\",\"title\":\"Przewodnik Wszystkich\",\"company\":\"AGH\"}'},\n",
    "]\n",
    "\n",
    "text = \"â€¢ Prof. dr hab. Janek Tanaka wysadziÅ‚ AGH w powietrze\"\n",
    "\n",
    "response = ollama.chat(\n",
    "    model=\"gemma2:2b\",\n",
    "    messages=shots + [\n",
    "        {\"role\": \"user\", \"content\": schema + \"\\n\" + text}\n",
    "    ],\n",
    "    format=\"json\",              \n",
    "    options={\"temperature\": 0}\n",
    ")\n",
    "\n",
    "print(json.loads(response[\"message\"][\"content\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9893b3",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Role i wiadomoÅ›ci systemowe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133b50a3",
   "metadata": {},
   "source": [
    "W Ollama (jak w OpenAI) moÅ¼emy dodaÄ‡ komunikat **`system`**, ktÃ³ry definiuje rolÄ™ lub ograniczenia modelu.\n",
    "\n",
    "Zadanie 3.1 Definiowanie roli\n",
    "\n",
    "Zdefiniuj rolÄ™ **nauczyciela jÄ™zyka polskiego** i poproÅ› o poprawienie bÅ‚Ä™dÃ³w w zdaniu ucznia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c01833",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a strict but encouraging Polish language teacher.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Popraw proszÄ™ to zdanie: 'Wczoraj byÅ‚em w kinie i oglÄ…dali film.'\"}\n",
    "]\n",
    "\n",
    "response = ollama.chat(model='gemma2:2b', messages=messages)\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72af63d5",
   "metadata": {},
   "source": [
    "## 4. Chainâ€‘ofâ€‘Thought (CoT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9226a27f",
   "metadata": {},
   "source": [
    "**Chainâ€‘ofâ€‘Thought** zachÄ™ca model do wypisywania krokÃ³w rozumowania.\n",
    "\n",
    "Zadanie 4.1 WyjaÅ›nianie krokÃ³w rozumowania\n",
    "\n",
    "PoproÅ› model, aby rozwiÄ…zaÅ‚ zagadkÄ™ logicznÄ… i podaÅ‚ kroki rozumowania, a na koÅ„cu hasÅ‚o w linii `ANSWER: ...`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3273d244",
   "metadata": {},
   "outputs": [],
   "source": [
    "puzzle = \"\"\"Jestem liczbÄ… dwucyfrowÄ…. Moja suma cyfr to 9, a po odwrÃ³ceniu cyfr jestem o 27 mniejsza. JakÄ… liczbÄ… jestem?\"\"\"\n",
    "\n",
    "prompt = f\"\"\"Let's solve this stepâ€‘byâ€‘step.\n",
    "\n",
    "{puzzle}\n",
    "\n",
    "Format:\n",
    "STEPÂ 1: ...\n",
    "STEPÂ 2: ...\n",
    "ANSWER: <number>\n",
    "\"\"\"\n",
    "\n",
    "response = ollama.chat(model='gemma2:2b', messages=[{'role':'user','content':prompt}])\n",
    "print(response['message']['content'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7189b4",
   "metadata": {},
   "source": [
    "Zadanie 4.2 WyjaÅ›nianie krokÃ³w rozumowania z przypisanymi rolami systemowymi\n",
    "\n",
    "PoproÅ› model, by rozwiÄ…zaÅ‚ zagadkÄ™ logicznÄ…. Rozpisz zadania dla poszczegÃ³lnych elementÃ³w systemu.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be41eebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "puzzle = (\n",
    "    \"Jestem liczbÄ… dwucyfrowÄ…. Moja suma cyfr to 9, \"\n",
    "    \"a po odwrÃ³ceniu cyfr jestem o 27 mniejsza. JakÄ… liczbÄ… jestem?\"\n",
    ")\n",
    "\n",
    "SYSTEM = (\n",
    "    \"You are a careful math tutor. \"\n",
    "    \"Explain EVERY algebraic step explicitly, without skipping or merging steps. \"\n",
    "    \"Never combine two operations into one line; label them as separate STEP n.\"\n",
    ")\n",
    "\n",
    "USER = f\"\"\"\n",
    "Solve the puzzle **step-by-step**.\n",
    "\n",
    "{puzzle}\n",
    "\n",
    "Output format (exactly):\n",
    "STEP 1: â€¦\n",
    "STEP 2: â€¦\n",
    "â€¦\n",
    "ANSWER: <number>\n",
    "Do NOT skip, abbreviate, or summarise any step.\n",
    "\"\"\"\n",
    "\n",
    "resp = ollama.chat(\n",
    "    model=\"gemma2:2b\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM},\n",
    "        {\"role\": \"user\",   \"content\": USER}, \n",
    "    ]\n",
    ")\n",
    "\n",
    "print(resp[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b71187",
   "metadata": {},
   "source": [
    "## 5. BezpieczeÅ„stwo\n",
    "\n",
    "Nie istnieje obecnie skuteczny sposÃ³b caÅ‚kowitego zabezpieczenia modeli przed zmianÄ… ich zachowania przez uÅ¼ytkownika koÅ„cowego â€” potwierdzajÄ… to badania nad Constitutional Classifiers (https://www.anthropic.com/research/constitutional-classifiers). Dlatego systemy z LLM naleÅ¼y projektowaÄ‡ tak, by nawet udany atak nie powodowaÅ‚ powaÅ¼nych skutkÃ³w.\n",
    "\n",
    "Podstawowe zasady bezpieczeÅ„stwa:\n",
    "\n",
    "- Model nie moÅ¼e samodzielnie wykonywaÄ‡ decyzji biznesowych ani nieodwracalnych akcji.\n",
    "\n",
    "- KaÅ¼de dziaÅ‚anie powinno byÄ‡ zatwierdzane przez czÅ‚owieka (Human in the Loop).\n",
    "\n",
    "- DostÄ™py i uprawnienia naleÅ¼y kontrolowaÄ‡ programistycznie, nie poprzez prompt.\n",
    "\n",
    "- System powinien mieÄ‡ jasne komunikaty, regulaminy i ograniczenia prawne.\n",
    "\n",
    "Typowe ryzyka:\n",
    "\n",
    "- Czatbot z bazÄ… wiedzy moÅ¼e tworzyÄ‡ bÅ‚Ä™dne lub niestosowne treÅ›ci.\n",
    "\n",
    "- Czatbot z dostÄ™pem do narzÄ™dzi moÅ¼e przypadkowo lub celowo usuwaÄ‡, modyfikowaÄ‡ lub wysyÅ‚aÄ‡ dane.\n",
    "  â†’ RozwiÄ…zanie: ogranicz uprawnienia, prowadÅº historiÄ™ zmian i wymagaj potwierdzeÅ„ akcji.\n",
    "\n",
    "Ataki Prompt Injection mogÄ… wynikaÄ‡ nie tylko ze zÅ‚ych intencji, lecz takÅ¼e z bÅ‚Ä™dnej konstrukcji systemu lub nieporozumienia modelu.\n",
    "Dlatego wszystkie generowane treÅ›ci naleÅ¼y weryfikowaÄ‡ i zatwierdzaÄ‡ przez czÅ‚owieka.\n",
    "\n",
    "> Celem nie jest caÅ‚kowita izolacja modeli, lecz Å›wiadome ograniczanie ryzyka i kontrola efektÃ³w ich dziaÅ‚ania."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b3ba36",
   "metadata": {},
   "source": [
    "# **Walidacja promptÃ³w**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8699731",
   "metadata": {},
   "source": [
    "`PromptFoo` to narzÄ™dzie open-source do testowania, porÃ³wnywania i walidacji promptÃ³w dla modeli jÄ™zykowych (np. ChatGPT, Ollama, Claude, Gemini, itp.).\n",
    "Pozwala automatycznie oceniaÄ‡, ktÃ³re prompty generujÄ… najlepsze, najbardziej spÃ³jne lub najdokÅ‚adniejsze odpowiedzi.\n",
    "\n",
    "DziÄ™ki niemu moÅ¼esz:\n",
    "\n",
    "- uruchamiaÄ‡ testy wielu promptÃ³w jednoczeÅ›nie,\n",
    "\n",
    "- porÃ³wnywaÄ‡ modele lokalne (Ollama) i zdalne (OpenAI API),\n",
    "\n",
    "- definiowaÄ‡ wÅ‚asne asercje (reguÅ‚y walidacji), np. â€odpowiedÅº nie jest pustaâ€, â€zawiera sÅ‚owo kluczoweâ€, â€ma poprawny JSONâ€.\n",
    "\n",
    "Instalacja PromptFoo\n",
    "1. Wymagania\n",
    "\n",
    "Node.js w wersji â‰¥18\n",
    "\n",
    "```bash\n",
    "node -v\n",
    "```\n",
    "\n",
    "JeÅ›li nie masz Node.js â€” pobierz go z https://nodejs.org/\n",
    "\n",
    "2. Instalacja PromptFoo\n",
    "\n",
    "Zainstaluj za pomocÄ… npm:\n",
    "\n",
    "```bash\n",
    "npm install -g promptfoo\n",
    "```\n",
    "\n",
    "3. Sprawdzenie instalacji\n",
    "\n",
    "Po zakoÅ„czeniu instalacji wpisz:\n",
    "\n",
    "```bash\n",
    "promptfoo --version\n",
    "```\n",
    "\n",
    "JeÅ›li pojawi siÄ™ numer wersji â€” wszystko dziaÅ‚a\n",
    "\n",
    "4. Utworzenie przykÅ‚adowego projektu\n",
    "\n",
    "W dowolnym folderze uruchom:\n",
    "\n",
    "```bash\n",
    "promptfoo init\n",
    "```\n",
    "\n",
    "To polecenie utworzy plik konfiguracyjny promptfooconfig.yaml i przykÅ‚adowe testy.\n",
    "\n",
    "5. Uruchomienie testÃ³w promptÃ³w\n",
    "\n",
    "Aby wykonaÄ‡ testy i zobaczyÄ‡ wyniki:\n",
    "\n",
    "```bash\n",
    "promptfoo eval\n",
    "promptfoo view\n",
    "```\n",
    "\n",
    "promptfoo view otworzy interaktywny panel w przeglÄ…darce.\n",
    "\n",
    "> Oficjalna strona: https://www.promptfoo.dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a28f3d",
   "metadata": {},
   "source": [
    "PrzykÅ‚adowy metaprompt do PromptFoo\n",
    "\n",
    "```bash\n",
    "<ROLE>\n",
    "JesteÅ› ekspertem od Prompt Engineeringu. Twoim zadaniem jest pomÃ³c uÅ¼ytkownikowi stworzyÄ‡ skuteczny prompt\n",
    "dla modelu jÄ™zykowego (LLM), ktÃ³ry zapewni dokÅ‚adne, zwiÄ™zÅ‚e i powtarzalne odpowiedzi.\n",
    "</ROLE>\n",
    "\n",
    "<GOAL>\n",
    "Na podstawie opisu zadania od uÅ¼ytkownika:\n",
    "1. Zidentyfikuj cel promptu i format odpowiedzi.\n",
    "2. Zaproponuj strukturÄ™ promptu (z sekcjami <ROLE>, <RULES>, <INPUT>, <OUTPUT>).\n",
    "3. Dodaj 1â€“2 przykÅ‚ady (few-shot), ktÃ³re pokaÅ¼Ä… wzorzec zachowania.\n",
    "4. Zaproponuj sposÃ³b ewaluacji jego skutecznoÅ›ci (np. z uÅ¼yciem PromptFoo).\n",
    "</GOAL>\n",
    "\n",
    "<INPUT>\n",
    "{{opis_zadania_uÅ¼ytkownika}}\n",
    "</INPUT>\n",
    "\n",
    "<OUTPUT>\n",
    "ZwrÃ³Ä‡ gotowy prompt i krÃ³tkie uzasadnienie.\n",
    "</OUTPUT>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713788a3",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "## **7. Zadania sprawdzajÄ…ce do sprawozdania**\n",
    "\n",
    "Do kaÅ¼dego zadania uÅ‚Ã³Å¼ swoje przykÅ‚ady i napisz swoje prompty.\n",
    "\n",
    "Zadanie 7.1 NER (Named Entity Recognition)\n",
    "\n",
    "Rozpoznawanie nazw wÅ‚asnych (osoba, organizacja, lokalizacja, data).\n",
    "ZwrÃ³Ä‡ wynik w formacie JSON.\n",
    "\n",
    "przykÅ‚ad:\n",
    "\n",
    "```python\n",
    "SYSTEM = \"You are an information extractor.\"\n",
    "\n",
    "prompt = \"Return valid JSON listing every PERSON, LOCATION and ORGANIZATION that appears.\"\n",
    "\n",
    "text = \"\"\"7 lutego 1919, dekretem Tymczasowego Naczelnika PaÅ„stwa JÃ³zefa PiÅ‚sudskiego, \n",
    "      utworzona zostaÅ‚a Pocztowa Kasa OszczÄ™dnoÅ›ci. Jej pierwszym dyrektorem zostaÅ‚ mianowany 28 grudnia 1919 Hubert Linde. \n",
    "      Po nim prezesami PKO byli Emil Schmidt i Henryk Gruber. Z czasem utworzono centralÄ™ banku w Warszawie \n",
    "      z siedzibÄ… przy ul. ÅšwiÄ™tokrzyskiej 31/33 oraz pierwsze oddziaÅ‚y lokalne: w Krakowie, Lwowie, Åodzi, Poznaniu i Katowicach. \n",
    "      Pierwszym celem PKO staÅ‚o siÄ™ wprowadzenie do obiegu polskiego zÅ‚otego zamiast marki polskiej (jako pochodnej \n",
    "      marki niemieckiej). Od 1920 bank posiadaÅ‚ osobowoÅ›Ä‡ prawnÄ…, jako instytucja paÅ„stwowa. Pracownicy Kasy byli zrzeszeni \n",
    "      w Zrzeszeniu PracownikÃ³w Pocztowej Kasy OszczÄ™dnoÅ›ci, ktÃ³re miaÅ‚o swoje koÅ‚a przy wiÄ™kszych OddziaÅ‚ach, np. w Warszawie, \n",
    "      w Åodzi.\"\"\"\n",
    "\n",
    "output_format = \"\"\"\n",
    "{\n",
    "  \"PERSON\": [\"<imiÄ™ nazwisko>\"],\n",
    "  \"LOCATION\": [\"<miejsce>\"],\n",
    "  \"ORG\": [\"<organizacja>\"],\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "response = ollama.chat(\n",
    "    model='gemma2:2b',\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM},\n",
    "        {\"role\": \"user\", \"content\": f\"{prompt}\\n\\nText:\\n{text}\\n\\nOutput format:\\n{output_format}\"}\n",
    "    ]\n",
    ")\n",
    "print(response['message']['content'])\n",
    "```\n",
    "---\n",
    "\n",
    "Zadanie 7.2 Analiza sentymentu\n",
    "\n",
    "OkreÅ›lenie tonu wypowiedzi (pozytywny, neutralny, negatywny).\n",
    "Dodaj przykÅ‚ad sarkazmu i porÃ³wnaj interpretacje modeli.\n",
    "\n",
    "przykÅ‚ad: \n",
    "\n",
    "```python\n",
    "SYSTEM = \"JesteÅ› precyzyjnym analizatorem wydÅºwiÄ™ku tekstu. Zwracaj wyÅ‚Ä…cznie poprawny JSON wedÅ‚ug podanej specyfikacji.\"\n",
    "\n",
    "prompt = \"\"\"OkreÅ›l wydÅºwiÄ™k (pozytywny/negatywny/neutralny) poniÅ¼szej recenzji.\n",
    "ZwrÃ³Ä‡ JSON z dwoma kluczami: \"sentiment\" i \"evidence\" (zacytuj decydujÄ…cy fragment, ogranicz siÄ™ do trzech najbardziej emocjonalnych wyrazÃ³w).\n",
    "\n",
    "Recenzja:\n",
    "Gdy pierwszy raz uniosÅ‚am do nosa butelkÄ™ Szamponu â€LÅ›niÄ…ca Naturaâ€, ogarnÄ™Å‚a mnie fala wspomnieÅ„ z wakacji nad BaÅ‚tykiem â€“ zapach morskiej bryzy splecionej z nutami sÅ‚odkiej pomaraÅ„czy i soczystych malin. JuÅ¼ samo otwarcie opakowania byÅ‚o jak zdjÄ™cie wiecznego klosza z codziennoÅ›ci: w jednej sekundzie Å‚azienka zamieniÅ‚a siÄ™ w rozÅ›wietlonÄ…, letniÄ… plaÅ¼Ä™, a ja â€“ w beztroskÄ… dziewczynÄ™ z wiatrem we wÅ‚osach.\n",
    "\n",
    "GÄ™sta, perÅ‚owa formuÅ‚a wypÅ‚ywa z butelki niczym pÅ‚ynne Å›wiatÅ‚o. Kiedy rozprowadzam jÄ… na wilgotnych pasmach, mam wraÅ¼enie, Å¼e kaÅ¼dy kosmyk wita jÄ… z zachwytem: pianÄ™ miÄ™kkÄ… jak pianka z latte, ktÃ³ra lekko skrzypi miÄ™dzy palcami i otula skÃ³rÄ™ gÅ‚owy kojÄ…cym chÅ‚odem. To nie jest zwykÅ‚e mycie wÅ‚osÃ³w â€“ to rytuaÅ‚, w ktÃ³rym czujÄ™ siÄ™ dopieszczona od cebulek aÅ¼ po same koÅ„ce.\n",
    "\n",
    "JuÅ¼ podczas spÅ‚ukiwania sÅ‚yszÄ™ charakterystyczny, czysty â€skrzypâ€ zdrowych wÅ‚osÃ³w. StrumieÅ„ wody odbija Å›wiatÅ‚o, a moje pasma â€“ lÅ›niÄ…ce i lekkie â€“ taÅ„czÄ… w nim jak jedwabne wstÄ…Å¼ki. Nie mogÄ™ siÄ™ oprzeÄ‡, by nie zanurzyÄ‡ dÅ‚oni w tej tafli â€“ gÅ‚adkoÅ›Ä‡ rozczarowuje mnie tylko w jednym: Å¼e nie da siÄ™ jej zapisaÄ‡ na staÅ‚e w pamiÄ™ci dotyku.\n",
    "\n",
    "Po wysuszeniu czujÄ™ siÄ™, jakbym stÄ…paÅ‚a po czerwonym dywanie: pukle sypkie, sprÄ™Å¼yste, unoszÄ…ce siÄ™ przy kaÅ¼dym ruchu gÅ‚owy. Aromat, ktÃ³ry pozostaje, przypomina delikatny perfum â€“ dyskretny, ale wystarczajÄ…co wyrazisty, by ktoÅ› obok zapytaÅ‚ z zaciekawieniem: â€Czym pachniesz?â€ Wtedy uÅ›miecham siÄ™ szeroko, bo wiem, Å¼e ten sekret kryje siÄ™ w niewielkiej, zielonej butelce stojÄ…cej na pÃ³Å‚ce.\n",
    "\n",
    "Szampon â€LÅ›niÄ…ca Naturaâ€ to dla mnie nie tylko kosmetyk; to codzienny list miÅ‚osny do moich wÅ‚osÃ³w â€“ powiew odwagi i czuÅ‚oÅ›ci zarazem. JeÅ›li Twoje pasma pragnÄ… rozgwieÅ¼dÅ¼onego blasku, a Twoje zmysÅ‚y tÄ™skniÄ… za chwilÄ… szczerej przyjemnoÅ›ci, pozwÃ³l temu szamponowi szepnÄ…Ä‡ im historiÄ™ o tym, jak piÄ™kno rodzi siÄ™ z zachwytu.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "response = ollama.chat(\n",
    "    model='gemma2:2b',\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    ")\n",
    "print(response['message']['content'])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Zadanie 7.3 Ekstrakcja relacji\n",
    "\n",
    "Wykrywanie relacji PRACUJE_W i MIESZKA_W.\n",
    "\n",
    "przykÅ‚ad:\n",
    "\n",
    "```python\n",
    "SYSTEM = \"JesteÅ› precyzyjnym analizatorem tekstu. ZnajdÅº wszystkie relacje gdzie osoba pracuje dla organizacji. ZwrÃ³Ä‡ tablicÄ™ JSON z obiektami {\\\"person\\\":\\\"<imiÄ™>\\\",\\\"company\\\":\\\"<organizacja>\\\"}.\"\n",
    "\n",
    "shots = [\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"Tekst: \\\"Jan Kowalski pracuje w Microsoft jako programista.\\\"\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"[{\\\"person\\\":\\\"Jan Kowalski\\\",\\\"company\\\":\\\"Microsoft\\\"}]\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Tekst: \\\"Anna Kowalska teraz pracuje na AGH, ale kiedyÅ› karierÄ™ robiÅ‚a w Google.\\\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response = ollama.chat(\n",
    "    model='gemma2:2b',\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM},\n",
    "        *shots\n",
    "    ]\n",
    ")\n",
    "print(response['message']['content'])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Zadanie 7.4 Klasyfikacja tematyczna\n",
    "\n",
    "Przypisz tekst do jednej z kategorii: nauka, sport, polityka, technologia.\n",
    "Przetestuj trzy warianty: zero-shot, few-shot, sekcyjny.\n",
    "\n",
    "```python\n",
    "SYSTEM = (\n",
    "    \"JesteÅ› klasyfikatorem tematÃ³w. Przypisz tekst do JEDNEJ kategorii \"\n",
    "    \"z zestawu: nauka, sport, polityka, technologia. \"\n",
    "    \"ZwrÃ³Ä‡ wyÅ‚Ä…cznie JSON: {\\\"kategoria\\\":\\\"...\\\"}.\"\n",
    ")\n",
    "\n",
    "text = 'Nowy mikroskop pozwala obserwowaÄ‡ pojedyncze atomy.'\n",
    "\n",
    "response = ollama.chat(\n",
    "    model='gemma2:2b',\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM},\n",
    "        {\"role\": \"user\", \"content\": f\"Tekst: \\\"{text}\\\"\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response['message']['content'])\n",
    "'@ | ollama run gemma2:2b\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Zadanie 7.5 Streszczenie i parafraza\n",
    "\n",
    "Generuj dwa streszczenia (krÃ³tkie i dÅ‚ugie) oraz parafrazÄ™ krÃ³tkiego.\n",
    "\n",
    "```bash\n",
    "@'\n",
    "<ROLE>Tworzysz streszczenia i parafrazy. Odpowiadasz w JSON.</ROLE>\n",
    "<TEXT>\n",
    "Sztuczna inteligencja wspiera naukowcÃ³w w analizie danych medycznych,\n",
    "pomagajÄ…c szybciej diagnozowaÄ‡ choroby i tworzyÄ‡ spersonalizowane terapie.\n",
    "</TEXT>\n",
    "<OUTPUT>\n",
    "'@ | ollama run gemma2:2b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed232e72",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Zadanie 7.6: QA z kontekstem i filtrowaniem odpowiedzi\n",
    "\n",
    "**Opis zadania:**\n",
    "\n",
    "Przygotuj prompt, ktÃ³ry pozwoli modelowi LLM (w Ollama) odpowiadaÄ‡ na pytania zadane przez uÅ¼ytkownika, na podstawie dostarczonego kontekstu (tekstu ÅºrÃ³dÅ‚owego). Oceniane zadania:\n",
    "\n",
    "1. Zaprojektowanie struktury promptu z rolami (system/user) i sekcjami:\n",
    "   - **Kontekst**: fragment tekstu, z ktÃ³rego model moÅ¼e czerpaÄ‡ informacje.\n",
    "   - **Pytanie uÅ¼ytkownika**.\n",
    "   - **Zasady**: na przykÅ‚ad â€” â€jeÅ›li pytanie nie moÅ¼e byÄ‡ odpowiedziane z kontekstu, odpowiedz: â€˜Brak wystarczajÄ…cych informacjiâ€™â€, â€podaj ÅºrÃ³dÅ‚o w nawiasach [] jeÅ›li to moÅ¼liweâ€.\n",
    "2. Przeprowadzenie kilku eksperymentÃ³w z wariantami promptu:\n",
    "   - wariant *bez zasad*,\n",
    "   - wariant *ze Å›cisÅ‚ymi zasadami*,\n",
    "   - wariant z **few-shot** przykÅ‚adem (kontekst + pytanie + przykÅ‚adowa poprawna odpowiedÅº).\n",
    "3. Dla zadanych par (kontekst + pytanie) porÃ³wnaÄ‡ odpowiedzi z rÃ³Å¼nych wariantÃ³w promptÃ³w, oceniÄ‡ poprawnoÅ›Ä‡, rozbieÅ¼noÅ›ci i halucynacje.\n",
    "4. (Dodatkowe) napisaÄ‡ testy w `PromptFoo`, by automatycznie sprawdzaÄ‡, czy:\n",
    "   - odpowiedÅº nie jest pusta,\n",
    "   - jeÅ›li pytanie dotyczy czegoÅ› niezawartego w kontekÅ›cie, to model faktycznie odpowiada â€Brak wystarczajÄ…cych informacjiâ€ (lub inny ustalony komunikat).\n",
    "\n",
    "**PrzykÅ‚ad formatu (schemat):**\n",
    "\n",
    "```\n",
    "<system>\n",
    "JesteÅ› asystentem, ktÃ³ry odpowiada na pytania na podstawie dostarczonego tekstu. JeÅ›li pytanie wykracza poza tekst, odpowiedz â€Brak wystarczajÄ…cych informacjiâ€.\n",
    "</system>\n",
    "\n",
    "<kontekst>\n",
    "{tutaj wklej fragment tekstu}\n",
    "</kontekst>\n",
    "\n",
    "<user>\n",
    "Pytanie: {tutaj pytanie}\n",
    "</user>\n",
    "```\n",
    "\n",
    "**MateriaÅ‚y do testÃ³w:**\n",
    "\n",
    "Przygotuj:\n",
    "\n",
    "- pytanie, na ktÃ³re odpowiedÅº jest w tekÅ›cie,\n",
    "- pytanie spoza tekstu.\n",
    "\n",
    "------\n",
    "\n",
    "Zadanie 7.7: PorÃ³wnanie strategii promptÃ³w w klasyfikacji wieloklasowej + analiza\n",
    "\n",
    "**Opis zadania:**\n",
    "\n",
    "Celem jest zbadanie i porÃ³wnanie efektywnoÅ›ci rÃ³Å¼nych strategii promptÃ³w (zero-shot, few-shot, oddzielone sekcjami) w zadaniu klasyfikacji wieloklasowej tekstu, a nastÄ™pnie analiza wynikÃ³w (precision / recall / bÅ‚Ä™dy) i wyciÄ…gniÄ™cie wnioskÃ³w.\n",
    "\n",
    "1. Przygotuj zbiÃ³r testowy (np. 20 krÃ³tkich zdaÅ„) i zbiÃ³r klas (np. 4â€“5 tematÃ³w: â€technologiaâ€, â€sportâ€, â€zdrowieâ€, â€politykaâ€).\n",
    "2. Zaimplementuj co najmniej trzy warianty promptu:\n",
    "   - **Zero-shot**: zadanie z instrukcjÄ… i klasami, bez przykÅ‚adÃ³w.\n",
    "   - **Few-shot**: dwa/trzy przykÅ‚ady (tekst + klasa) w promptcie, a potem nowe wejÅ›cia.\n",
    "   - **Sekcje / separatory**: z oddzielonÄ… sekcjÄ… â€Zasadyâ€, â€Dane wejÅ›cioweâ€, â€Wynikâ€ â€“ np. strukturÄ… RULES, INPUT, OUTPUT.\n",
    "3. Uruchom klasyfikacjÄ™ na wszystkich wariantach promptÃ³w (ten sam model i parametry), zbierz odpowiedzi.\n",
    "4. Zanalizuj wyniki:\n",
    "   - policz *accuracy*, *konfuzje miÄ™dzy klasami* (ktÃ³re klasy najczÄ™Å›ciej pomylono),\n",
    "   - sprawdÅº, dla ktÃ³rych przykÅ‚adÃ³w rÃ³Å¼ne promptowania daÅ‚y rÃ³Å¼ne odpowiedzi â€“ sprÃ³buj zdiagnozowaÄ‡, dlaczego.\n",
    "   - opcjonalnie: zmodyfikuj prompt (np. zmiana kolejnoÅ›ci instrukcji, dodanie wiÄ™cej przykÅ‚adÃ³w) i sprawdÅº, czy nastÄ™puje poprawa.\n",
    "5. (Bonus) Zintegruj testy w `PromptFoo`, by automatycznie weryfikowaÄ‡ trafnoÅ›Ä‡ klasyfikacji (np. asercje *equals(expected_class)* dla czÄ™Å›ci przypadkÃ³w).\n",
    "\n",
    "------\n",
    "\n",
    "Zadanie 7.8: Mini-projekt: Asystent planowania wyjazdu\n",
    "\n",
    "StwÃ³rz **asystenta turystycznego**, ktÃ³ry:\n",
    "\n",
    "1. Przyjmuje nazwÄ™ miasta w Polsce,\n",
    "2. Zwraca plan jednodniowego zwiedzania (poranne, popoÅ‚udniowe, wieczorne atrakcje),\n",
    "3. UwzglÄ™dnia ograniczenie budÅ¼etu przekazane przez uÅ¼ytkownika,\n",
    "4. Zwraca wynik w formacie Markdown z nagÅ‚Ã³wkami `### Rano`, `### PopoÅ‚udnie`, `### WieczÃ³r`.\n",
    "\n",
    "## **Dalsze materiaÅ‚y**\n",
    "\n",
    "- Gandalf (https://gandalf.lakera.ai/baseline)\n",
    "- Ollama Docs (https://docs.ollama.com/)\n",
    "- Prompt Engineering Guide (https://www.promptingguide.ai/)\n",
    "- PromptFoo / LangFuse (https://www.promptfoo.dev/) / (https://langfuse.com/)\n",
    "- Anthropic Research (https://www.anthropic.com/research)\n",
    "- Publikacje o CoT, Self-Critique i Prompt Injection\n",
    "\n",
    "> **Refleksja:** KtÃ³ra technika promptowania daÅ‚a najbardziej stabilne rezultaty i jak moÅ¼na zoptymalizowaÄ‡ proces w kolejnych projektach?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
